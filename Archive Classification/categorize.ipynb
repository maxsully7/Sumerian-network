{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict, Counter\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DREHEM_IDS = 'clean_drehem_ids.txt'\n",
    "QUEEN_ARCHIVES_IDS = 'queen_archives_pids.txt'\n",
    "QUEEN_OIP_IDS = 'oip_pids.txt'\n",
    "\n",
    "labels = {}\n",
    "labels[\"domesticated_animal\"] = [\"[ox]\", \"[cow]\", \"[sheep]\", \"[goat]\", \"[lamb]\", \"[~sheep]\", \"[equid]\"] # account for plural\n",
    "labels[\"wild_animal\"] = [\"[bear]\", \"[gazelle]\", \"[mountain]\"] # account for \"mountain animal\" and plural\n",
    "labels[\"dead_animal\"] = [\"[die]\"] # find \"die\" before finding domesticated or wild\n",
    "labels[\"leather_object\"] = [\"[boots]\", \"[sandals]\"]\n",
    "labels[\"precious_object\"] = [\"[copper]\", \"[bronze]\", \"[silver]\", \"[gold]\"]\n",
    "labels[\"wool\"] = [\"[wool]\", \"[~wool]\"]\n",
    "labels[\"queens_archive\"] = []\n",
    "\n",
    "class Transaction:\n",
    "    def __init__(self, p_id):\n",
    "        self.p_id = p_id\n",
    "        self.lines = list()\n",
    "        self.lemmas = OrderedDict() # Maps Sumerian text to its lemmatized form\n",
    "        self.label = {} # Maps label to List of defining text\n",
    "        self.sumerian_lemmas = []\n",
    "        \n",
    "    # Create mapping of Sumerian text to its lemmatized form\n",
    "    def get_lemmatization(self):\n",
    "        first_line = 0\n",
    "        for i, s in enumerate(self.lines):\n",
    "            if s.startswith(\"1.\"):\n",
    "                  first_line = i\n",
    "                  break\n",
    "        while first_line < len(self.lines)-1:\n",
    "            if self.lines[first_line] and self.lines[first_line][0].isnumeric() and self.lines[first_line+1].startswith(\"#lem\"):\n",
    "                self.lemmas[self.lines[first_line]] = self.lines[first_line+1]\n",
    "                first_line += 2\n",
    "            else:\n",
    "                first_line += 1\n",
    "                \n",
    "        return self.lemmas\n",
    "    \n",
    "    # Get Sumerian lemmatized text only\n",
    "    def get_sumerian_lemma(self):\n",
    "        #print(item.sumerian_lemmas)\n",
    "        item.sumerian_lemmas = []\n",
    "        for k, v in self.lemmas.items():\n",
    "            #print(v)\n",
    "            result = re.findall(\" .*\\[[a-z]+\\]\", v)\n",
    "            if len(result) == 0:\n",
    "                continue\n",
    "            lemmas = [s[:s.index(\"[\")].strip() for s in result[0].split(\";\") if re.search(\"\\[\", s)]\n",
    "            self.sumerian_lemmas += lemmas\n",
    "        return self.sumerian_lemmas\n",
    "    \n",
    "    # Find the most likely label\n",
    "    def set_label(self):\n",
    "        def find_label(label, line, found) :\n",
    "            for val in labels[label]:\n",
    "                if val in line: \n",
    "                    if label in found.keys():\n",
    "                        found[label].append(line)\n",
    "                    else:\n",
    "                        found[label] = [line]\n",
    "                    return True\n",
    "        found = {}\n",
    "        for line in self.lines:\n",
    "            label = None\n",
    "            if line == '@object seal':\n",
    "                found['seal'] = [line]\n",
    "            # Priority 1: Check for dead animal\n",
    "            if find_label(\"dead_animal\", line, found): continue\n",
    "            # Priority 2: Check for wild animal\n",
    "            if find_label(\"wild_animal\", line, found): continue\n",
    "            # Priority 3: Check for domesticated animal\n",
    "            if find_label(\"domesticated_animal\", line, found): continue\n",
    "            # Priority 4: Check leather, wool, or precious object\n",
    "            if find_label(\"leather_object\", line, found): continue\n",
    "            if find_label(\"precious_object\", line, found): continue\n",
    "            if find_label(\"wool\", line, found): break\n",
    "        # If none match, label as \"Unknown\"\n",
    "        if len(found.keys()) == 0:\n",
    "            found[\"Unknown\"] = [self.lines]\n",
    "        self.label = found\n",
    "        return found\n",
    "            \n",
    "    \n",
    "# Read ORACC files to find transactions with p_ids in `ids`\n",
    "def read_files(subdir, ids, reverse=False):\n",
    "    transactions = list()\n",
    "    for i in range(1, 16):\n",
    "        file_name = \"\"\n",
    "        if i < 10:\n",
    "            file_name += subdir + \"p00\" + str(i) + \".atf\"\n",
    "        else:\n",
    "            file_name += subdir + \"p0\" + str(i) + \".atf\"\n",
    "        \n",
    "        curr_transaction = None\n",
    "        \n",
    "        with open(file_name, encoding=\"utf8\") as file:\n",
    "            print(\"Opening:\", file_name)\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                if line.startswith('&P'):\n",
    "                    p_id = line.split()[0][1:]\n",
    "                    if (not reverse and p_id in ids):\n",
    "                        ids.remove(p_id)\n",
    "                        if curr_transaction:\n",
    "                            transactions.append(curr_transaction)\n",
    "                        transaction = Transaction(p_id)\n",
    "                        curr_transaction = transaction\n",
    "                    elif (reverse and p_id not in ids and len(transactions) <= 200):\n",
    "                        if curr_transaction:\n",
    "                            transactions.append(curr_transaction)\n",
    "                        transaction = Transaction(p_id)\n",
    "                        curr_transaction = transaction\n",
    "                    else:\n",
    "                        if curr_transaction:\n",
    "                            transactions.append(curr_transaction)\n",
    "                        curr_transaction = None\n",
    "                else:\n",
    "                    if curr_transaction:\n",
    "                        curr_transaction.lines.append(line)\n",
    "        \n",
    "        if curr_transaction:\n",
    "            transactions.append(curr_transaction)\n",
    "    \n",
    "    #print(ids)\n",
    "    #assert len(ids) == 0\n",
    "    print(\"Number of transactions:\", len(transactions))\n",
    "    return transactions\n",
    "\n",
    "# Return the IDs of docs to annotate\n",
    "def get_drehem_ids(file):\n",
    "    lst = list()\n",
    "    with open(file, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"P\"):\n",
    "                line = line.strip()\n",
    "                lst.append(line)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening: raw-data/p001.atf\n",
      "Opening: raw-data/p002.atf\n",
      "Opening: raw-data/p003.atf\n",
      "Opening: raw-data/p004.atf\n",
      "Opening: raw-data/p005.atf\n",
      "Opening: raw-data/p006.atf\n",
      "Opening: raw-data/p007.atf\n",
      "Opening: raw-data/p008.atf\n",
      "Opening: raw-data/p009.atf\n",
      "Opening: raw-data/p010.atf\n",
      "Opening: raw-data/p011.atf\n",
      "Opening: raw-data/p012.atf\n",
      "Opening: raw-data/p013.atf\n",
      "Opening: raw-data/p014.atf\n",
      "Opening: raw-data/p015.atf\n",
      "Number of transactions: 429\n",
      "Opening: raw-data/p001.atf\n",
      "Opening: raw-data/p002.atf\n",
      "Opening: raw-data/p003.atf\n",
      "Opening: raw-data/p004.atf\n",
      "Opening: raw-data/p005.atf\n",
      "Opening: raw-data/p006.atf\n",
      "Opening: raw-data/p007.atf\n",
      "Opening: raw-data/p008.atf\n",
      "Opening: raw-data/p009.atf\n",
      "Opening: raw-data/p010.atf\n",
      "Opening: raw-data/p011.atf\n",
      "Opening: raw-data/p012.atf\n",
      "Opening: raw-data/p013.atf\n",
      "Opening: raw-data/p014.atf\n",
      "Opening: raw-data/p015.atf\n",
      "Number of transactions: 275\n",
      "Opening: raw-data/p001.atf\n",
      "Opening: raw-data/p002.atf\n",
      "Opening: raw-data/p003.atf\n",
      "Opening: raw-data/p004.atf\n",
      "Opening: raw-data/p005.atf\n",
      "Opening: raw-data/p006.atf\n",
      "Opening: raw-data/p007.atf\n",
      "Opening: raw-data/p008.atf\n",
      "Opening: raw-data/p009.atf\n",
      "Opening: raw-data/p010.atf\n",
      "Opening: raw-data/p011.atf\n",
      "Opening: raw-data/p012.atf\n",
      "Opening: raw-data/p013.atf\n",
      "Opening: raw-data/p014.atf\n",
      "Opening: raw-data/p015.atf\n",
      "Number of transactions: 120\n"
     ]
    }
   ],
   "source": [
    "list_drehem_ids = get_drehem_ids(DREHEM_IDS)\n",
    "list_queen_ids = get_drehem_ids(QUEEN_ARCHIVES_IDS)\n",
    "list_oip_queen_ids = get_drehem_ids(QUEEN_OIP_IDS)\n",
    "complete_list = list_drehem_ids + list_queen_ids + list_oip_queen_ids\n",
    "\n",
    "non_queen_list = read_files(\"raw-data/\", list_drehem_ids)\n",
    "queen_training_list = read_files(\"raw-data/\", list_queen_ids)\n",
    "queen_test_set = read_files(\"raw-data/\", list_oip_queen_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525\n",
      "525\n",
      "299\n",
      "299\n"
     ]
    }
   ],
   "source": [
    "# Populate training and data set\n",
    "\n",
    "for item in queen_training_list:\n",
    "    item.get_lemmatization()\n",
    "    item.set_label()\n",
    "    \n",
    "for item in non_queen_list:\n",
    "    item.get_lemmatization()\n",
    "    item.set_label()\n",
    "    \n",
    "for item in queen_test_set:\n",
    "    item.get_lemmatization()\n",
    "    item.set_label()\n",
    "    \n",
    "            \n",
    "training_data = []\n",
    "training_sumerian_data = []\n",
    "training_labels = []\n",
    "test_data = []\n",
    "test_sumerian_data = []\n",
    "test_labels = []\n",
    "\n",
    "for item in queen_training_list[:175]:\n",
    "    training_data.append(\" \".join(item.get_sumerian_lemma()))\n",
    "    txt = [x[3:] for x in item.lemmas.keys()]\n",
    "    training_sumerian_data.append(\" \".join(txt))\n",
    "    training_labels.append(\"queen\")\n",
    "    \n",
    "for i in range(len(non_queen_list)):\n",
    "    if i < 350:\n",
    "        training_data.append(\" \".join(non_queen_list[i].get_sumerian_lemma()))\n",
    "        txt = [x[3:] for x in non_queen_list[i].lemmas.keys()]\n",
    "        training_sumerian_data.append(\" \".join(txt))\n",
    "        training_labels.append(\"not queen\")\n",
    "    else:\n",
    "        test_data.append(\" \".join(non_queen_list[i].get_sumerian_lemma()))\n",
    "        txt = [x[3:] for x in non_queen_list[i].lemmas.keys()]\n",
    "        test_sumerian_data.append(\" \".join(txt))\n",
    "        test_labels.append(\"not queen\")\n",
    "        \n",
    "for item in queen_test_set:\n",
    "    test_data.append(\" \".join(item.get_sumerian_lemma()))\n",
    "    txt = [x[3:] for x in item.lemmas.keys()]\n",
    "    test_sumerian_data.append(\" \".join(txt))\n",
    "    test_labels.append(\"queen\")\n",
    "    \n",
    "for item in queen_training_list[175:]:\n",
    "    test_data.append(\" \".join(item.get_sumerian_lemma()))\n",
    "    txt = [x[3:] for x in item.lemmas.keys()]\n",
    "    test_sumerian_data.append(\" \".join(txt))\n",
    "    test_labels.append(\"queen\")\n",
    "\n",
    "print(len(training_data))\n",
    "print(len(training_labels))\n",
    "print(len(test_data))\n",
    "print(len(test_labels))\n",
    "\n",
    "all_data = training_data + test_data\n",
    "all_sumerian_data = training_sumerian_data + test_sumerian_data\n",
    "all_labels = training_labels + test_labels\n",
    "\n",
    "queen_data = [x for x, y in zip(all_data, all_labels) if y == \"queen\"]\n",
    "# queen_data = [x for x, y in zip(all_sumerian_data, all_labels) if y == \"queen\"]\n",
    "queen_labels = [\"queen\"] * len(queen_data)\n",
    "non_queen_data = [x for x, y in zip(all_data, all_labels) if y == \"not queen\"]\n",
    "# non_queen_data = [x for x, y in zip(all_sumerian_data, all_labels) if y == \"not queen\"]\n",
    "non_queen_labels = [\"not queen\"] * len(non_queen_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes Classifer\n",
    "First pass at classifying Queen's Archives transactions.\n",
    "\n",
    "We will use the following measures to determine how well our classifier does.\n",
    "\n",
    "<b>Accuracy</b>: \n",
    "(# true positives + # true negatives) / total #<br><br>\n",
    "<b>Recall</b>:\n",
    "true positives / (true positives + false positives) <br>\n",
    "High recall means that an algorithm returned most of the relevant results <br><br>\n",
    "<b>Precision</b>:\n",
    "true positives / (true positives + false negatives) <br>\n",
    "High precision means that an algorithm returned substantially more relevant results than irrelevant ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(525, 368)\n",
      "Accuracy:  0.8595317725752508\n",
      "Recall:  0.9004890678941312\n",
      "Precision:  0.824953314659197\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words model (Unigram)\n",
    "count_vect = CountVectorizer(analyzer = \"word\",\n",
    "                                          tokenizer = None,    \n",
    "                                          preprocessor = None,\n",
    "                                          ngram_range = (1, 1),\n",
    "                                          binary = False,\n",
    "                                          strip_accents='unicode',\n",
    "                                          token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "\n",
    "# Train\n",
    "X_train_counts = count_vect.fit_transform(training_data) # Learn the vocabulary dictionary and return term-document matrix.\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "# Get TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Classifier\n",
    "bag_of_words_classifier = MultinomialNB().fit(X_train_tfidf, training_labels)\n",
    "\n",
    "# Predict\n",
    "X_new_counts = count_vect.transform(test_data)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = bag_of_words_classifier.predict(X_new_tfidf)\n",
    "    \n",
    "print(\"Accuracy: \", np.mean(predicted == test_labels))\n",
    "print(\"Recall: \", str(metrics.recall_score(test_labels, predicted, [\"queen\", \"not queen\"], average=\"macro\")))\n",
    "print(\"Precision: \", str(metrics.precision_score(test_labels, predicted, [\"queen\", \"not queen\"], average=\"macro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(525, 2476)\n",
      "Accuracy:  0.8996655518394648\n",
      "Recall:  0.9237054085155351\n",
      "Precision:  0.861512027491409\n"
     ]
    }
   ],
   "source": [
    "# Bigram Model\n",
    "bigram_vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                                    tokenizer = None,\n",
    "                                    preprocessor = None,\n",
    "                                    ngram_range = (2, 2),\n",
    "                                    strip_accents='unicode',\n",
    "                                    token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "\n",
    "# Train\n",
    "X_train_counts = bigram_vectorizer.fit_transform(training_data) # Learn the vocabulary dictionary and return term-document matrix.\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "# Get TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Classifier\n",
    "bigram_classifier = MultinomialNB().fit(X_train_tfidf, training_labels)\n",
    "\n",
    "# Predict\n",
    "X_new_counts = bigram_vectorizer.transform(test_data)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "bigram_multinomial_nb_prediction = bigram_classifier.predict(X_new_tfidf)\n",
    "\n",
    "print(\"Accuracy: \", np.mean(bigram_multinomial_nb_prediction == test_labels))\n",
    "print(\"Recall: \", str(metrics.recall_score(test_labels, bigram_multinomial_nb_prediction, [\"queen\", \"not queen\"], average=\"macro\")))\n",
    "print(\"Precision: \", str(metrics.precision_score(test_labels, bigram_multinomial_nb_prediction, [\"queen\", \"not queen\"], average=\"macro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(525, 4711)\n",
      "Accuracy:  0.8929765886287625\n",
      "Recall:  0.9151035673187572\n",
      "Precision:  0.8541728031418754\n"
     ]
    }
   ],
   "source": [
    "# Trigram Model\n",
    "trigram_vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                                    tokenizer = None,\n",
    "                                    preprocessor = None,\n",
    "                                    ngram_range = (3, 3),\n",
    "                                    strip_accents='unicode',\n",
    "                                    token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "\n",
    "# Train\n",
    "X_train_counts = trigram_vectorizer.fit_transform(training_data) # Learn the vocabulary dictionary and return term-document matrix.\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "# Get TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Classifier\n",
    "trigram_classifier = MultinomialNB().fit(X_train_tfidf, training_labels)\n",
    "\n",
    "# Predict\n",
    "X_new_counts = trigram_vectorizer.transform(test_data)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "trigram_prediction = trigram_classifier.predict(X_new_tfidf)\n",
    "\n",
    "print(\"Accuracy: \", np.mean(trigram_prediction == test_labels))\n",
    "print(\"Recall: \", str(metrics.recall_score(test_labels, trigram_prediction, [\"queen\", \"not queen\"], average=\"macro\")))\n",
    "print(\"Precision: \", str(metrics.precision_score(test_labels, trigram_prediction, [\"queen\", \"not queen\"], average=\"macro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(525, 2844)\n",
      "Accuracy:  0.9163879598662207\n",
      "Recall:  0.9350690448791714\n",
      "Precision:  0.8799748743718593\n"
     ]
    }
   ],
   "source": [
    "# Unigram and Bigram Model\n",
    "uni_and_bigram_vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                                            tokenizer = None,\n",
    "                                            preprocessor = None,\n",
    "                                            binary = False,\n",
    "                                            ngram_range = (1,2),\n",
    "                                            strip_accents='unicode',\n",
    "                                            token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "\n",
    "# Train\n",
    "X_train_counts = uni_and_bigram_vectorizer.fit_transform(training_data) # Learn the vocabulary dictionary and return term-document matrix.\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "# Get TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Classifier\n",
    "uni_and_bigram_classifier = MultinomialNB(0.5).fit(X_train_tfidf, training_labels)\n",
    "\n",
    "# Predict\n",
    "X_new_counts = uni_and_bigram_vectorizer.transform(test_data)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "uni_and_bigram_prediction = uni_and_bigram_classifier.predict(X_new_tfidf)\n",
    "\n",
    "print(\"Accuracy: \", np.mean(uni_and_bigram_prediction == test_labels))\n",
    "print(\"Recall: \", str(metrics.recall_score(test_labels, uni_and_bigram_prediction, [\"queen\", \"not queen\"], average=\"macro\")))\n",
    "print(\"Precision: \", str(metrics.precision_score(test_labels, uni_and_bigram_prediction, [\"queen\", \"not queen\"], average=\"macro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we can see the unigram-bigram model does the best. This is the one we will use later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do a preliminary prediction using just the (unigram-bigram) Naive Bayes Classifier. All files will be read in to get a tentative result for the size of the Queen's Archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all files to get all Drehem transactions\n",
    "def read_files(subdir, ids, reverse=False):\n",
    "    transactions = list()\n",
    "    for i in range(1, 16):\n",
    "        file_name = \"\"\n",
    "        if i < 10:\n",
    "            file_name += subdir + \"p00\" + str(i) + \".atf\"\n",
    "        else:\n",
    "            file_name += subdir + \"p0\" + str(i) + \".atf\"\n",
    "        \n",
    "        curr_transaction = None\n",
    "        \n",
    "        with open(file_name, encoding=\"utf8\") as file:\n",
    "            print(\"Opening:\", file_name)\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                if line.startswith('&P'):\n",
    "                    p_id = line.split()[0][1:]\n",
    "                    #print(p_id)\n",
    "                    if (not reverse and p_id in ids):\n",
    "                        ids.remove(p_id)\n",
    "                        if curr_transaction:\n",
    "                            transactions.append(curr_transaction)\n",
    "                        transaction = Transaction(p_id)\n",
    "                        curr_transaction = transaction\n",
    "                    elif (reverse and p_id not in ids and len(transactions) <= 200):\n",
    "                        if curr_transaction:\n",
    "                            transactions.append(curr_transaction)\n",
    "                        transaction = Transaction(p_id)\n",
    "                        curr_transaction = transaction\n",
    "                    else:\n",
    "                        if curr_transaction:\n",
    "                            transactions.append(curr_transaction)\n",
    "                        curr_transaction = None\n",
    "                else:\n",
    "                    if curr_transaction:\n",
    "                        curr_transaction.lines.append(line)\n",
    "        \n",
    "        if curr_transaction:\n",
    "            transactions.append(curr_transaction)\n",
    "    \n",
    "    print(\"Number of transactions:\", len(transactions))\n",
    "    return transactions\n",
    "\n",
    "# Return the IDs of docs to annotate\n",
    "def get_drehem_ids(file):\n",
    "    lst = list()\n",
    "    with open(file, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            lst.append(\"P\" + line)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening: raw-data/p001.atf\n",
      "Opening: raw-data/p002.atf\n",
      "Opening: raw-data/p003.atf\n",
      "Opening: raw-data/p004.atf\n",
      "Opening: raw-data/p005.atf\n",
      "Opening: raw-data/p006.atf\n",
      "Opening: raw-data/p007.atf\n",
      "Opening: raw-data/p008.atf\n",
      "Opening: raw-data/p009.atf\n",
      "Opening: raw-data/p010.atf\n",
      "Opening: raw-data/p011.atf\n",
      "Opening: raw-data/p012.atf\n",
      "Opening: raw-data/p013.atf\n",
      "Opening: raw-data/p014.atf\n",
      "Opening: raw-data/p015.atf\n",
      "Number of transactions: 14594\n"
     ]
    }
   ],
   "source": [
    "all_ids = get_drehem_ids(\"drehem_p_ids.txt\")\n",
    "all_transactions = read_files(\"raw-data/\", all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "sumerian_data = []\n",
    "mapping = {}\n",
    "sumerian_mapping = {}\n",
    "labs = []\n",
    "\n",
    "for t in all_transactions:\n",
    "    t.get_lemmatization()\n",
    "    labs.append(t.set_label())\n",
    "    lemma = \" \".join(t.get_sumerian_lemma())\n",
    "    data.append(lemma)\n",
    "    txt = [x[3:] for x in t.lemmas.keys()]\n",
    "    sumerian_text = \" \".join(txt)\n",
    "    sumerian_data.append(sumerian_text)\n",
    "    if lemma in mapping:\n",
    "        mapping[lemma].append(t)\n",
    "    else:\n",
    "        mapping[lemma] = [t]\n",
    "    if sumerian_text in sumerian_mapping:\n",
    "        sumerian_mapping[sumerian_text].append(t)\n",
    "    else:\n",
    "        sumerian_mapping[sumerian_text] = [t]\n",
    "        \n",
    "# Predict\n",
    "X_new_counts = uni_and_bigram_vectorizer.transform(data)\n",
    "# X_new_counts = uni_and_bigram_vectorizer.transform(sumerian_data)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "uni_and_bigram_prediction = uni_and_bigram_classifier.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'udu kišib ki lugal kalag lugal lugal an anubda limmu dubsar dumu arad' => not queen\n",
      "'udu niga sila ga uš ud ki šu teŋ itud mu u hulu' => not queen\n",
      "'sila sila mu.DU zabardab maškim u udu maš uš ekišibak ud lal ki itud mu us hulu' => not queen\n",
      "'mašgal niga udu uš ud šag ki šu teŋ itud mu lugal' => not queen\n",
      "'udu a sag udu niga mašgal niga udu sila ga kir ga uš ud ki šu teŋ itud mu lugal' => not queen\n",
      "'udu niga sila niga udu sila ensik sila ensik sila mu.DU itud mu en maš pad ud' => not queen\n",
      "'mašgal niga egia ensik ragaba maškim itud ud lal zal ki ŋiri dubsar itud mu en huŋ udu' => not queen\n",
      "'amar mašda mašda amar amar mašda sila amar mašda maš sila ensik mu.DU dab itud gu mu us hulu ud' => not queen\n",
      "'mašgal ki ensik dab itud mu huŋ udu' => not queen\n",
      "'sila zabardab sila ensik sila ensik mu.DU dab itud akiti mu u mada ud hulu ud' => not queen\n",
      "'mašgal niga lu maškim itud ud zal ki itud mu lugal hulu' => not queen\n",
      "'gud ab mu.DU lugal ki bala zig ensik dab ziga itud mu en maš pad' => not queen\n",
      "'lal udu udu u u sila gub sila gub šugid udu a gukkal lal maš gub uzud ašgar ki ensik ud mu.DU dab itud mu du' => not queen\n",
      "'udu sadug šag udu uš mu sipad šu teŋ ki itud mu us hulu' => not queen\n",
      "'udu sadug šag udu u maš uš mu urgir sipad urgir šu teŋ ki itud mu us hulu' => not queen\n",
      "'udunita u kir ur itud uzud kir ur udunita mašgal itud akiti sadug u mu e du' => queen\n",
      "'udu u mašgal u uš sadug urgir sipad urgir šu teŋ ugula ki ŋiri dubsar itud mu lugal dim udu' => not queen\n",
      "'sila sila mu.DU sila sila mu.DU en sila mu.DU zabardab maškim gud niga u uš emuhaldim ud ziga itud mu en maš pad' => not queen\n",
      "'udu mašgal udu mašgal šagia maškim šag mu.DU ud ki ŋiri dubsar itud mu du' => not queen\n",
      "'šugid emuhaldim mu gardu sukkal maškim itud ud zal ki itud mu en huŋ' => not queen\n",
      "'u uzud šugid emuhaldim mu gardu maškim šag ašag ud ki ŋiri dubsar itud mu en huŋ' => not queen\n",
      "'durah munus sila sila sila sila sila ud mu.DU dab itud mu huŋ' => not queen\n",
      "'sila ud ki dab itud mu huŋ' => not queen\n",
      "'sila mu.DU zabardab maškim ud ki itud mu u mada ud hulu' => not queen\n",
      "'mašda ekišibak mu.DU ud ki itud mu u mada ud hulu' => not queen\n",
      "'mašgal babbar ud ki dab ŋiri itud mu huŋ udu' => not queen\n",
      "'sila ga kir ga maš ga ašgar ga utuda ud ki itud mu lugal' => not queen\n",
      "'udu niga udu aslum ud lal ki dab itud mu huŋ' => not queen\n",
      "'udu niga sag us gukkal niga u niga udu uzud sila kir uš ud ki šu teŋ itud mu lugal' => not queen\n",
      "'sila ga kir ga maš ga ašgar ga utuda ud šag dab itud mu lugal' => not queen\n"
     ]
    }
   ],
   "source": [
    "# Small sample of results\n",
    "for doc, category in zip(data[:30], uni_and_bigram_prediction[:30]):\n",
    "    print('%r => %s' % (doc, category))\n",
    "# for doc, category in zip(sumerian_data[:30], uni_and_bigram_prediction[:30]):\n",
    "#     print('%r => %s' % (doc, category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05543373989310676\n"
     ]
    }
   ],
   "source": [
    "# Percentange of Queen transactions\n",
    "print(len([i for i in uni_and_bigram_prediction if i == \"queen\"])/len(uni_and_bigram_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try to improve our classification by using an SVM, or a Support Vector Machine. We'll use `GridSearchCV` to determine the best hyperparameters for our model.\n",
    "\n",
    "We will use the following hyperparameters:\n",
    "\n",
    "**Kernel:** A transformation function; takes in data and transforms it into different forms. \n",
    "\n",
    "The kernels we will use are linear and RBF (radial basis function).\n",
    "RBF kernels are useful when there is no prior knowledge about the data.\n",
    "\n",
    "**Gamma:** Influence of a single sample.\n",
    "\n",
    "A low `gamma` value means a single sample can have a very large influence on the classifier; a high `gamma` value means its influence is more limited.\n",
    "\n",
    "**C:** Regularization parameter - trades off between model simplicity and classification accuracy.\n",
    "\n",
    "A low `C` value means a very smooth decision surface (easy to explain); a high `C` value means high accuracy but possible overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-classification Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we want to use all of our training data to train our model for the best accuracy. Because we won't be holding anything back for validation, we should make sure that this is appropriate first; given our data, the model should be able to generalize well to other data.\n",
    "\n",
    "The following section is very intensive and will take a long time to run (roughly 30 minutes for this dataset) but only needs to be run once. Here we do a k-fold cross-validation with 10 folds. For each of 10 iterations, 9 sections are used as training for a model and one section is held out for validation. For each section, we make sure to include the same proportion of queen and non-queen samples as with the overall data.\n",
    "\n",
    "After running this, visually inspect the results and make sure that the results for each fold are similar; that is, there is no section where results drop dramatically, as this would indicate that the data does not generalize well to other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold 0\n",
      "Best parameters found:\n",
      "{'C': 5, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Starting fold 1\n",
      "Best parameters found:\n",
      "{'C': 5, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "Starting fold 2\n",
      "Best parameters found:\n",
      "{'C': 1, 'gamma': 0.5, 'kernel': 'rbf'}\n",
      "Starting fold 3\n",
      "Best parameters found:\n",
      "{'C': 5, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Starting fold 4\n",
      "Best parameters found:\n",
      "{'C': 5, 'gamma': 1, 'kernel': 'rbf'}\n",
      "Starting fold 5\n",
      "Best parameters found:\n",
      "{'C': 5, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Starting fold 6\n",
      "Best parameters found:\n",
      "{'C': 10, 'gamma': 0.05, 'kernel': 'rbf'}\n",
      "Starting fold 7\n",
      "Best parameters found:\n",
      "{'C': 1, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "Starting fold 8\n",
      "Best parameters found:\n",
      "{'C': 5, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Starting fold 9\n",
      "Best parameters found:\n",
      "{'C': 1, 'gamma': 0.0001, 'kernel': 'linear'}\n",
      "Total Accuracy:  0.9514563106796117\n",
      "Average Accuracy:  0.9517089018843405\n",
      "Total Recall:  0.9507715912779204\n",
      "Average Recall:  0.9510029186499775\n",
      "Total Precision:  0.9522447795480247\n",
      "Average Precision:  0.9537631113274667\n"
     ]
    }
   ],
   "source": [
    "# Only run this section once\n",
    "\n",
    "n1 = len(queen_data)\n",
    "n2 = len(non_queen_data)\n",
    "q_fold_size = [i * (n1 // 10) for i in range(10)] + [n1] \n",
    "nq_fold_size = [i * (n2 // 10) for i in range(10)] + [n2] \n",
    "all_pred = []\n",
    "all_true_lab = []\n",
    "acc = []\n",
    "rec = []\n",
    "prec = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"Starting fold\", i)\n",
    "    q_start, q_end = q_fold_size[i], q_fold_size[i + 1]\n",
    "    nq_start, nq_end = nq_fold_size[i], nq_fold_size[i + 1]\n",
    "    test_data = queen_data[q_start:q_end] + non_queen_data[nq_start:nq_end]\n",
    "    test_labels = queen_labels[q_start:q_end] + non_queen_labels[nq_start:nq_end]\n",
    "    training_data = queen_data[:q_start] + queen_data[q_end:] + non_queen_data[:nq_start] + non_queen_data[nq_end:]\n",
    "    training_labels = queen_labels[:q_start] + queen_labels[q_end:] + non_queen_labels[:nq_start] + non_queen_labels[nq_end:]\n",
    "\n",
    "    uni_bi_vect = CountVectorizer(analyzer = \"word\",\n",
    "                                  ngram_range = (1,2),\n",
    "                                  token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "\n",
    "    # Train\n",
    "    X_train_counts = uni_bi_vect.fit_transform(training_data)\n",
    "\n",
    "    # Get TF-IDF\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "    # Classifier\n",
    "    params = [{'kernel': ['rbf', 'linear'],\n",
    "              'gamma': [1e-4, 1e-3, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 100],\n",
    "              'C': [1, 5, 10, 50]}]\n",
    "    uni_bi_clf = GridSearchCV(svm.SVC(decision_function_shape='ovr'), params)\n",
    "    uni_bi_clf.fit(X_train_tfidf, training_labels)\n",
    "    print(\"Best parameters found:\")\n",
    "    print(uni_bi_clf.best_params_)\n",
    "\n",
    "    # Predict\n",
    "    X_new_counts = uni_bi_vect.transform(test_data)\n",
    "    X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "    all_pred.extend(uni_bi_clf.predict(X_new_tfidf))\n",
    "    all_true_lab.extend(test_labels)\n",
    "    \n",
    "    uni_bi_pred = uni_bi_clf.predict(X_new_tfidf)\n",
    "\n",
    "    acc.append(np.mean(uni_bi_pred == test_labels))\n",
    "    rec.append(metrics.recall_score(test_labels, uni_bi_pred, [\"queen\", \"not queen\"], average=\"macro\"))\n",
    "    prec.append(metrics.precision_score(test_labels, uni_bi_pred, [\"queen\", \"not queen\"], average=\"macro\"))\n",
    "\n",
    "print(\"Total Accuracy: \", sum([1 if all_pred[i] == all_true_lab[i] else 0 for i in range(len(all_pred))])/len(all_pred))\n",
    "print(\"Average Accuracy: \", sum(acc) / len(acc))\n",
    "print(\"Total Recall: \", str(metrics.recall_score(all_true_lab, all_pred, [\"queen\", \"not queen\"], average=\"macro\")))\n",
    "print(\"Average Recall: \", sum(rec) / len(rec))\n",
    "print(\"Total Precision: \", str(metrics.precision_score(all_true_lab, all_pred, [\"queen\", \"not queen\"], average=\"macro\")))\n",
    "print(\"Average Precision: \", sum(prec) / len(prec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the generalizability of the data is verified, we will use all of our pre-labeled data to train the model. The parameters of our model will be chosen based on the best-results from the verification above. Here, the best parameters seem to be `C` = 1, `gamma` = 0.1, and `kernel` = 'rbf'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Queens' Texts Predicted\n",
      "0.06838426750719474\n"
     ]
    }
   ],
   "source": [
    "uni_bi_vect = CountVectorizer(analyzer = \"word\",\n",
    "                                  ngram_range = (1,2),\n",
    "                                  token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "\n",
    "# Train\n",
    "X_train_counts = uni_bi_vect.fit_transform(all_data) \n",
    "\n",
    "# Get TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Classifier\n",
    "\n",
    "uni_bi_clf = svm.SVC(decision_function_shape='ovr', C=5, gamma=0.1, kernel='rbf')\n",
    "uni_bi_clf.fit(X_train_tfidf, all_labels)\n",
    "\n",
    "X_all_counts = uni_bi_vect.transform(data) \n",
    "X_all_tfidf = tfidf_transformer.transform(X_all_counts)\n",
    "uni_bi_pred_v2 = uni_bi_clf.predict(X_all_tfidf)\n",
    "print(\"Percentage Queens' Texts Predicted\")\n",
    "print(len([i for i in uni_bi_pred_v2 if i == 'queen'])/len(uni_bi_pred_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13596\n"
     ]
    }
   ],
   "source": [
    "queen_data = [x for x, y in zip(data, uni_bi_pred_v2) if y == \"queen\"]\n",
    "non_queens = [x for x, y in zip(data, uni_bi_pred_v2) if y == \"not queen\"]\n",
    "sumerian_queen_data = [x for x, y in zip(sumerian_data, uni_bi_pred_v2) if y == \"queen\"]\n",
    "sumerian_non_queens = [x for x, y in zip(sumerian_data, uni_bi_pred_v2) if y == \"not queen\"]\n",
    "non_queen_labs = [x for x, y in zip(labs, uni_bi_pred_v2) if y == \"not queen\"]\n",
    "print(len(non_queens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating into Archives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our classifier, we'll separate the data into archives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dead Animals archive size: 2470\n",
      "Domesticated Animals archive size: 8507\n",
      "Leather Objects archive size: 44\n",
      "Precious Objects archive size: 230\n",
      "Wild Animals archive size: 324\n",
      "Wool archive size: 66\n",
      "Seal archive size: 620\n",
      "Unknown archive size: 1335\n"
     ]
    }
   ],
   "source": [
    "dead_animal_archive = []\n",
    "domesticated_animal_archive = []\n",
    "wild_animal_archive = []\n",
    "leather_object_archive = []\n",
    "precious_object_archive = []\n",
    "wool_archive = []\n",
    "seal_archive = []\n",
    "Unknown_archive = []\n",
    "\n",
    "dead_animal_sumerian_archive = []\n",
    "domesticated_animal_sumerian_archive = []\n",
    "wild_animal_sumerian_archive = []\n",
    "leather_object_sumerian_archive = []\n",
    "precious_object_sumerian_archive = []\n",
    "wool_sumerian_archive = []\n",
    "seal_sumerian_archive = []\n",
    "Unknown_sumerian_archive = []\n",
    "\n",
    "Unknown_labs = []\n",
    "\n",
    "for x, y, z in zip(non_queens, non_queen_labs, sumerian_non_queens):\n",
    "    if \"seal\" in y.keys():\n",
    "        seal_archive.append(x)\n",
    "        seal_sumerian_archive.append(z)\n",
    "    else:\n",
    "        lab = max(y, key = lambda x: len(y[x]))\n",
    "        if lab == \"Unknown\":\n",
    "            Unknown_labs.append(list(y.values()))\n",
    "        if \"animal\" in lab and \"dead_animal\" in y.keys():\n",
    "            dead_animal_archive.append(x)\n",
    "            dead_animal_sumerian_archive.append(z)\n",
    "        else:\n",
    "            exec(lab + \"_archive.append(x)\")\n",
    "            exec(lab + \"_sumerian_archive.append(z)\")\n",
    "      \n",
    "print(\"Dead Animals archive size:\", len(dead_animal_archive))\n",
    "print(\"Domesticated Animals archive size:\", len(domesticated_animal_archive))\n",
    "print(\"Leather Objects archive size:\", len(leather_object_archive))\n",
    "print(\"Precious Objects archive size:\", len(precious_object_archive))\n",
    "print(\"Wild Animals archive size:\", len(wild_animal_archive))\n",
    "print(\"Wool archive size:\", len(wool_archive))\n",
    "print(\"Seal archive size:\", len(seal_archive))\n",
    "print(\"Unknown archive size:\", len(Unknown_archive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Phase 2: Stage by Stage Archiving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we classified the text by certain labels found within the lemmatizations. Now, we will try to combine these results with a second approach. \n",
    "\n",
    "We will take advantage of the Sumerian texts, which contain information like names and locations that were stripped away in the lemmatizations. We will use the above classified texts as training data to create models based on the Sumerian text and use that to see if we can further classify our Unknown archive.\n",
    "\n",
    "This approach will take step in several stages, one for each archive (so that we will be doing binary classification and not multi-class classification). We will do the stages in the following order:\n",
    "\n",
    "* Domesticated Animals\n",
    "* Wild Animals\n",
    "* Dead Animals\n",
    "* Precious Objects\n",
    "* Wool\n",
    "* Leather Objects\n",
    "\n",
    "This was decided mainly by the apparent sizes of the archives, with an exception made to classify wild animals before classifying dead animals.\n",
    "\n",
    "At each stage, we will use the texts classified as a specific archive above as the positive training data and use all other texts (not in that archive) as the negative training data. In many cases, we choose a subset of classified texts instead of all available classified texts so that the positive class and negative class in our training data is roughly even. The model is trained on this testing data, allowing us to classify texts as part of the archive or not in the archive. The texts that are classified as not being in that archive will be used in the next stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Domesticated Animals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "3134\n"
     ]
    }
   ],
   "source": [
    "domesticated_sumerian = domesticated_animal_sumerian_archive[:3000]\n",
    "domesticated_labels = [\"domesticated animal\"] * len(domesticated_sumerian)\n",
    "non_domesticated_sumerian = dead_animal_sumerian_archive + leather_object_sumerian_archive + precious_object_sumerian_archive \\\n",
    "                                + wild_animal_sumerian_archive + wool_sumerian_archive\n",
    "non_domesticated_labels = [\"not domesticated animal\"] * len(non_domesticated_sumerian)\n",
    "\n",
    "stage_1_data = domesticated_sumerian + non_domesticated_sumerian\n",
    "stage_1_labels = domesticated_labels + non_domesticated_labels\n",
    "\n",
    "print(len(domesticated_labels))\n",
    "print(len(non_domesticated_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Domesticated Animals Archive Texts Predicted\n",
      "0.8134831460674158\n"
     ]
    }
   ],
   "source": [
    "uni_bi_vect = CountVectorizer(analyzer = \"word\",\n",
    "                                  ngram_range = (1,2),\n",
    "                                  token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "\n",
    "# Train\n",
    "X_train_counts = uni_bi_vect.fit_transform(stage_1_data) \n",
    "\n",
    "# Get TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Classifier\n",
    "uni_bi_clf = svm.SVC(decision_function_shape='ovr', C=5, gamma=0.1, kernel='rbf')\n",
    "uni_bi_clf.fit(X_train_tfidf, stage_1_labels)\n",
    "\n",
    "X_all_counts = uni_bi_vect.transform(Unknown_sumerian_archive) \n",
    "X_all_tfidf = tfidf_transformer.transform(X_all_counts)\n",
    "uni_bi_pred_v2 = uni_bi_clf.predict(X_all_tfidf)\n",
    "print(\"Percentage Domesticated Animals Archive Texts Predicted\")\n",
    "print(len([i for i in uni_bi_pred_v2 if i == 'domesticated animal'])/len(uni_bi_pred_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1086\n",
      "249\n"
     ]
    }
   ],
   "source": [
    "domest_animal_data = [x for x, y in zip(Unknown_sumerian_archive, uni_bi_pred_v2) if y == \"domesticated animal\"]\n",
    "non_domest_animal = [x for x, y in zip(Unknown_sumerian_archive, uni_bi_pred_v2) if y == \"not domesticated animal\"]\n",
    "print(len(domest_animal_data))\n",
    "print(len(non_domest_animal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Wild Animals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324\n",
      "410\n"
     ]
    }
   ],
   "source": [
    "wild_sumerian = wild_animal_sumerian_archive\n",
    "wild_labels = [\"wild animal\"] * len(wild_sumerian)\n",
    "non_wild_sumerian = domesticated_animal_sumerian_archive[:100] + leather_object_sumerian_archive + precious_object_sumerian_archive[:100] \\\n",
    "                                + dead_animal_sumerian_archive[:100] + wool_sumerian_archive\n",
    "non_wild_labels = [\"not wild animal\"] * len(non_wild_sumerian)\n",
    "\n",
    "stage_2_data = wild_sumerian + non_wild_sumerian\n",
    "stage_2_labels = wild_labels + non_wild_labels\n",
    "\n",
    "print(len(wild_labels))\n",
    "print(len(non_wild_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Wild Animals Archive Texts Predicted\n",
      "0.14859437751004015\n"
     ]
    }
   ],
   "source": [
    "uni_bi_vect = CountVectorizer(analyzer = \"word\",\n",
    "                                  ngram_range = (1,2),\n",
    "                                  token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "\n",
    "# Train\n",
    "X_train_counts = uni_bi_vect.fit_transform(stage_2_data) \n",
    "\n",
    "# Get TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Classifier\n",
    "uni_bi_clf = svm.SVC(decision_function_shape='ovr', C=5, gamma=0.1, kernel='rbf')\n",
    "uni_bi_clf.fit(X_train_tfidf, stage_2_labels)\n",
    "\n",
    "X_all_counts = uni_bi_vect.transform(non_domest_animal) \n",
    "X_all_tfidf = tfidf_transformer.transform(X_all_counts)\n",
    "uni_bi_pred_v2 = uni_bi_clf.predict(X_all_tfidf)\n",
    "print(\"Percentage Wild Animals Archive Texts Predicted\")\n",
    "print(len([i for i in uni_bi_pred_v2 if i == 'wild animal'])/len(uni_bi_pred_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "212\n"
     ]
    }
   ],
   "source": [
    "wild_animal_data = [x for x, y in zip(non_domest_animal, uni_bi_pred_v2) if y == \"wild animal\"]\n",
    "non_wild_animal = [x for x, y in zip(non_domest_animal, uni_bi_pred_v2) if y == \"not wild animal\"]\n",
    "print(len(wild_animal_data))\n",
    "print(len(non_wild_animal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 3: Dead Animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2164\n"
     ]
    }
   ],
   "source": [
    "dead_sumerian = dead_animal_sumerian_archive[:2000]\n",
    "dead_labels = [\"dead animal\"] * len(dead_sumerian)\n",
    "non_dead_sumerian = domesticated_animal_sumerian_archive[:1500] + leather_object_sumerian_archive + precious_object_sumerian_archive \\\n",
    "                                + wild_animal_sumerian_archive + wool_sumerian_archive\n",
    "non_dead_labels = [\"not dead animal\"] * len(non_dead_sumerian)\n",
    "\n",
    "stage_3_data = dead_sumerian + non_dead_sumerian\n",
    "stage_3_labels = dead_labels + non_dead_labels\n",
    "\n",
    "print(len(dead_labels))\n",
    "print(len(non_dead_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Dead Animals Archive Texts Predicted\n",
      "0.014150943396226415\n"
     ]
    }
   ],
   "source": [
    "uni_bi_vect = CountVectorizer(analyzer = \"word\",\n",
    "                                  ngram_range = (1,2),\n",
    "                                  token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "\n",
    "# Train\n",
    "X_train_counts = uni_bi_vect.fit_transform(stage_3_data) \n",
    "\n",
    "# Get TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Classifier\n",
    "uni_bi_clf = svm.SVC(decision_function_shape='ovr', C=5, gamma=0.1, kernel='rbf')\n",
    "uni_bi_clf.fit(X_train_tfidf, stage_3_labels)\n",
    "\n",
    "X_all_counts = uni_bi_vect.transform(non_wild_animal) \n",
    "X_all_tfidf = tfidf_transformer.transform(X_all_counts)\n",
    "uni_bi_pred_v2 = uni_bi_clf.predict(X_all_tfidf)\n",
    "print(\"Percentage Dead Animals Archive Texts Predicted\")\n",
    "print(len([i for i in uni_bi_pred_v2 if i == 'dead animal'])/len(uni_bi_pred_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "209\n"
     ]
    }
   ],
   "source": [
    "dead_animal_data = [x for x, y in zip(non_wild_animal, uni_bi_pred_v2) if y == \"dead animal\"]\n",
    "non_dead_animal = [x for x, y in zip(non_wild_animal, uni_bi_pred_v2) if y == \"not dead animal\"]\n",
    "print(len(dead_animal_data))\n",
    "print(len(non_dead_animal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 4: Precious Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230\n",
      "260\n"
     ]
    }
   ],
   "source": [
    "precious_sumerian = precious_object_sumerian_archive\n",
    "precious_labels = [\"precious object\"] * len(precious_sumerian)\n",
    "non_precious_sumerian = domesticated_animal_sumerian_archive[:50] + leather_object_sumerian_archive + dead_animal_sumerian_archive[:50] \\\n",
    "                                + wild_animal_sumerian_archive[:50] + wool_sumerian_archive\n",
    "non_precious_labels = [\"not precious object\"] * len(non_precious_sumerian)\n",
    "\n",
    "stage_4_data = precious_sumerian + non_precious_sumerian\n",
    "stage_4_labels = precious_labels + non_precious_labels\n",
    "\n",
    "print(len(precious_labels))\n",
    "print(len(non_precious_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Precious Objects Archive Texts Predicted\n",
      "0.5023923444976076\n"
     ]
    }
   ],
   "source": [
    "uni_bi_vect = CountVectorizer(analyzer = \"word\",\n",
    "                                  ngram_range = (1,2),\n",
    "                                  token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "\n",
    "# Train\n",
    "X_train_counts = uni_bi_vect.fit_transform(stage_4_data) \n",
    "\n",
    "# Get TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Classifier\n",
    "uni_bi_clf = svm.SVC(decision_function_shape='ovr', C=5, gamma=0.1, kernel='rbf')\n",
    "uni_bi_clf.fit(X_train_tfidf, stage_4_labels)\n",
    "\n",
    "X_all_counts = uni_bi_vect.transform(non_dead_animal) \n",
    "X_all_tfidf = tfidf_transformer.transform(X_all_counts)\n",
    "uni_bi_pred_v2 = uni_bi_clf.predict(X_all_tfidf)\n",
    "print(\"Percentage Precious Objects Archive Texts Predicted\")\n",
    "print(len([i for i in uni_bi_pred_v2 if i == 'precious object'])/len(uni_bi_pred_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "104\n"
     ]
    }
   ],
   "source": [
    "precious_object_data = [x for x, y in zip(non_dead_animal, uni_bi_pred_v2) if y == \"precious object\"]\n",
    "non_precious_object = [x for x, y in zip(non_dead_animal, uni_bi_pred_v2) if y == \"not precious object\"]\n",
    "print(len(precious_object_data))\n",
    "print(len(non_precious_object))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 5: Wool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "wool_sumerian = wool_sumerian_archive\n",
    "wool_labels = [\"wool\"] * len(wool_sumerian)\n",
    "non_wool_sumerian = domesticated_animal_sumerian_archive[:30] + leather_object_sumerian_archive[:30] + dead_animal_sumerian_archive[:30] \\\n",
    "                                + wild_animal_sumerian_archive[:30] + precious_object_sumerian_archive[:30]\n",
    "non_wool_labels = [\"not wool\"] * len(non_wool_sumerian)\n",
    "\n",
    "stage_5_data = wool_sumerian + non_wool_sumerian\n",
    "stage_5_labels = wool_labels + non_wool_labels\n",
    "\n",
    "print(len(wool_labels))\n",
    "print(len(non_wool_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Wool Archive Texts Predicted\n",
      "0.47115384615384615\n"
     ]
    }
   ],
   "source": [
    "uni_bi_vect = CountVectorizer(analyzer = \"word\",\n",
    "                                  ngram_range = (1,2),\n",
    "                                  token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "\n",
    "# Train\n",
    "X_train_counts = uni_bi_vect.fit_transform(stage_5_data) \n",
    "\n",
    "# Get TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Classifier\n",
    "uni_bi_clf = svm.SVC(decision_function_shape='ovr', C=5, gamma=0.1, kernel='rbf')\n",
    "uni_bi_clf.fit(X_train_tfidf, stage_5_labels)\n",
    "\n",
    "X_all_counts = uni_bi_vect.transform(non_precious_object) \n",
    "X_all_tfidf = tfidf_transformer.transform(X_all_counts)\n",
    "uni_bi_pred_v2 = uni_bi_clf.predict(X_all_tfidf)\n",
    "print(\"Percentage Wool Archive Texts Predicted\")\n",
    "print(len([i for i in uni_bi_pred_v2 if i == 'wool'])/len(uni_bi_pred_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "wool_data = [x for x, y in zip(non_precious_object, uni_bi_pred_v2) if y == \"wool\"]\n",
    "non_wool = [x for x, y in zip(non_precious_object, uni_bi_pred_v2) if y == \"not wool\"]\n",
    "print(len(wool_data))\n",
    "print(len(non_wool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 6: Leather Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "leather_sumerian = leather_object_sumerian_archive\n",
    "leather_labels = [\"leather object\"] * len(leather_sumerian)\n",
    "non_leather_sumerian = domesticated_animal_sumerian_archive[:20] + precious_object_sumerian_archive[:20] + dead_animal_sumerian_archive[:20] \\\n",
    "                                + wild_animal_sumerian_archive[:20] + wool_sumerian_archive[:20]\n",
    "non_leather_labels = [\"not leather object\"] * len(non_leather_sumerian)\n",
    "\n",
    "stage_6_data = leather_sumerian + non_leather_sumerian\n",
    "stage_6_labels = leather_labels + non_leather_labels\n",
    "\n",
    "print(len(leather_labels))\n",
    "print(len(non_leather_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Leather Objects Archive Texts Predicted\n",
      "0.2909090909090909\n"
     ]
    }
   ],
   "source": [
    "uni_bi_vect = CountVectorizer(analyzer = \"word\",\n",
    "                                  ngram_range = (1,2),\n",
    "                                  token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "\n",
    "# Train\n",
    "X_train_counts = uni_bi_vect.fit_transform(stage_6_data) \n",
    "\n",
    "# Get TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Classifier\n",
    "uni_bi_clf = svm.SVC(decision_function_shape='ovr', C=5, gamma=0.1, kernel='rbf')\n",
    "uni_bi_clf.fit(X_train_tfidf, stage_6_labels)\n",
    "\n",
    "X_all_counts = uni_bi_vect.transform(non_wool) \n",
    "X_all_tfidf = tfidf_transformer.transform(X_all_counts)\n",
    "uni_bi_pred_v2 = uni_bi_clf.predict(X_all_tfidf)\n",
    "print(\"Percentage Leather Objects Archive Texts Predicted\")\n",
    "print(len([i for i in uni_bi_pred_v2 if i == 'leather object'])/len(uni_bi_pred_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "leather_object_data = [x for x, y in zip(non_wool, uni_bi_pred_v2) if y == \"leather object\"]\n",
    "unknowns = [x for x, y in zip(non_wool, uni_bi_pred_v2) if y == \"not leather object\"]\n",
    "print(len(leather_object_data))\n",
    "print(len(unknowns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Classification Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dead Animals archive size: 3\n",
      "Domesticated Animals archive size: 1086\n",
      "Leather Objects archive size: 16\n",
      "Precious Objects archive size: 105\n",
      "Wild Animals archive size: 37\n",
      "Wool archive size: 49\n",
      "Unknown archive size: 39\n"
     ]
    }
   ],
   "source": [
    "print(\"Dead Animals archive size:\", len(dead_animal_data))\n",
    "print(\"Domesticated Animals archive size:\", len(domest_animal_data))\n",
    "print(\"Leather Objects archive size:\", len(leather_object_data))\n",
    "print(\"Precious Objects archive size:\", len(precious_object_data))\n",
    "print(\"Wild Animals archive size:\", len(wild_animal_data))\n",
    "print(\"Wool archive size:\", len(wool_data))\n",
    "print(\"Unknown archive size:\", len(unknowns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Unknown Labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will do a brief visual inspection of terms found in texts that were classified as Unknown in our lemmatization-based SVM approach. This will help us see if we have missed any terms that can be used for further classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_lists = [list(x.values())[0] for x in non_queen_labs]\n",
    "lines = []\n",
    "for lem in lem_lists:\n",
    "    for line in lem:\n",
    "        if line[:5] == \"#lem:\":\n",
    "            lines.append(line)\n",
    "line_string = \"\".join(lines)\n",
    "translations = [t.strip() for t in re.split(\";|#lem:|\\|\", line_string) if t != \"\" and len(t) > 2 and \"[\" in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trans_dict = {}\n",
    "for tr in translations:\n",
    "    word, defn = re.split(\"\\[\", tr)\n",
    "    if defn[-1] == \"]\":\n",
    "        defn = defn[:-1]\n",
    "    if defn in trans_dict.keys():\n",
    "        trans_dict[defn].append(word)\n",
    "    else:\n",
    "        trans_dict[defn] = [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking what words fall under the Unknown category\n",
    "\n",
    "Unknown_lines = [x[0][0] for x in Unknown_labs]\n",
    "Unknown_lemmas = []\n",
    "for l in Unknown_lines:\n",
    "    for val in l:\n",
    "        if val[:5] == \"#lem:\":\n",
    "            Unknown_lemmas.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_string_unknown = \"\".join(Unknown_lemmas)\n",
    "translations_unknown = list(set([t.strip() for t in re.split(\";|#lem:|\\|\", line_string_unknown) if t != \"\" and len(t) > 2 and \"[\" in t]))\n",
    "# print(translations_unknown)\n",
    "\n",
    "for tr in translations_unknown:\n",
    "    word, defn = re.split(\"\\[\", tr)\n",
    "    if defn[-1] == \"]\":\n",
    "        defn = defn[:-1]\n",
    "    if defn in trans_dict.keys():\n",
    "        trans_dict[defn].append(word)\n",
    "    else:\n",
    "        trans_dict[defn] = [word]\n",
    "        \n",
    "for k, v in trans_dict.items():\n",
    "    trans_dict[k] = list(set(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_tagged_words = re.findall(\"\\[(.*?)\\]\", \" \".join(Unknown_lemmas))\n",
    "non_tagged_freq = Counter(non_tagged_words)\n",
    "sorted_words_by_freq = sorted(list(non_tagged_freq.keys()), key = lambda x: (-non_tagged_freq[x], x))\n",
    "words_with_sumerian = [[x, trans_dict[x]] for x in sorted_words_by_freq]\n",
    "# print(words_with_sumerian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_words = open('unknown_text_words.csv', 'w', encoding='utf-8')\n",
    "unknown_words.write(\"Word, Frequency, List of Sumerian Words\\n\")\n",
    "for w, defs in words_with_sumerian:\n",
    "    unknown_words.write(w + \", \" + str(non_tagged_freq[w]) + \", \" + str(defs) + \"\\n\")\n",
    "unknown_words.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Texts and Archive Labels to IDs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that are texts are classified, we can create a final CSV mapping each PID to an archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_archive_map = {}\n",
    "    \n",
    "for text in dead_animal_archive:\n",
    "    t_list = mapping[text]\n",
    "    for t in t_list:\n",
    "        id_archive_map[t.p_id] = \"dead animal\"\n",
    "\n",
    "for text in wild_animal_archive:\n",
    "    t_list = mapping[text]\n",
    "    for t in t_list:\n",
    "        id_archive_map[t.p_id] = \"wild animal\"\n",
    "    \n",
    "for text in domesticated_animal_archive:\n",
    "    t_list = mapping[text]\n",
    "    for t in t_list:\n",
    "        id_archive_map[t.p_id] = \"domesticated animal\"\n",
    "        \n",
    "for text in leather_object_archive:\n",
    "    t_list = mapping[text]\n",
    "    for t in t_list:\n",
    "        id_archive_map[t.p_id] = \"leather object\"\n",
    "    \n",
    "for text in precious_object_archive:\n",
    "    t_list = mapping[text]\n",
    "    for t in t_list:\n",
    "        id_archive_map[t.p_id] = \"precious object\"\n",
    "\n",
    "for text in wool_archive:\n",
    "    t_list = mapping[text]\n",
    "    for t in t_list:\n",
    "        id_archive_map[t.p_id] = \"wool\"\n",
    "    \n",
    "for text in seal_archive:\n",
    "    t_list = mapping[text]\n",
    "    for t in t_list:\n",
    "        id_archive_map[t.p_id] = \"seal\"\n",
    "        \n",
    "for text in Unknown_archive:\n",
    "    t_list = mapping[text]\n",
    "    for t in t_list:\n",
    "        id_archive_map[t.p_id] = \"unknown\"\n",
    "\n",
    "for text in queen_data:\n",
    "    t_list = mapping[text]\n",
    "    for t in t_list:\n",
    "        id_archive_map[t.p_id] = \"queen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumerian_id_archive_map = {}\n",
    "    \n",
    "for text in dead_animal_data:\n",
    "    t_list = sumerian_mapping[text]\n",
    "    for t in t_list:\n",
    "        sumerian_id_archive_map[t.p_id] = \"dead animal\"\n",
    "\n",
    "for text in wild_animal_data:\n",
    "    t_list = sumerian_mapping[text]\n",
    "    for t in t_list:\n",
    "        sumerian_id_archive_map[t.p_id] = \"wild animal\"\n",
    "    \n",
    "for text in domest_animal_data:\n",
    "    t_list = sumerian_mapping[text]\n",
    "    for t in t_list:\n",
    "        sumerian_id_archive_map[t.p_id] = \"domesticated animal\"\n",
    "        \n",
    "for text in leather_object_data:\n",
    "    t_list = sumerian_mapping[text]\n",
    "    for t in t_list:\n",
    "        sumerian_id_archive_map[t.p_id] = \"leather object\"\n",
    "    \n",
    "for text in precious_object_data:\n",
    "    t_list = sumerian_mapping[text]\n",
    "    for t in t_list:\n",
    "        sumerian_id_archive_map[t.p_id] = \"precious object\"\n",
    "\n",
    "for text in wool_data:\n",
    "    t_list = sumerian_mapping[text]\n",
    "    for t in t_list:\n",
    "        sumerian_id_archive_map[t.p_id] = \"wool\"\n",
    "    \n",
    "for text in unknowns:\n",
    "    t_list = sumerian_mapping[text]\n",
    "    for t in t_list:\n",
    "        sumerian_id_archive_map[t.p_id] = \"unknown\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results of archive classification to CSV file\n",
    "\n",
    "archive = open('archive_map.csv', 'w', encoding='utf-8')\n",
    "archive.write(\"PID,Archive\\n\")\n",
    "for k, v in id_archive_map.items():\n",
    "    archive.write(k + \",\" + v + \"\\n\")\n",
    "archive.close()\n",
    "\n",
    "sumerian_archive = open('sumerian_archive_map.csv', 'w', encoding='utf-8')\n",
    "sumerian_archive.write(\"PID,Archive\\n\")\n",
    "for k, v in sumerian_id_archive_map.items():\n",
    "    sumerian_archive.write(k + \",\" + v + \"\\n\")\n",
    "sumerian_archive.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sample Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a random sample of our results from the lemmatization-based SVM so that we can do our own manual inspection to verify that there is nothing clearly suspicious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ids = [id[1:] for id in complete_list]\n",
    "all_texts_ids = list(id_archive_map.keys())\n",
    "non_training_ids = [id for id in all_texts_ids if id not in training_ids]\n",
    "\n",
    "random_indices = random.sample(range(len(non_training_ids)), 50)\n",
    "random_ids = [non_training_ids[index] for index in range(len(non_training_ids)) if index in random_indices]\n",
    "\n",
    "random_sample_ids = []\n",
    "random_sample_texts = []\n",
    "random_sample_labels = []\n",
    "\n",
    "for text, t_list in mapping.items():\n",
    "    for t in t_list:\n",
    "        if t.p_id in random_ids:\n",
    "            random_sample_ids.append(t.p_id)\n",
    "            random_sample_texts.append(text)\n",
    "            random_sample_labels.append(id_archive_map[t.p_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results of random sample to CSV file\n",
    "\n",
    "sample = open('random_sample.csv', 'w', encoding='utf-8')\n",
    "sample.write(\"PID,Text,Archive\\n\")\n",
    "for i in range(len(random_sample_ids)):\n",
    "# for i in range(1):\n",
    "    sample.write(random_sample_ids[i] + \",\" + random_sample_texts[i] + \",\" + random_sample_labels[i] + \"\\n\")\n",
    "sample.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage by Stage Classification Sample Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a random sample of our results from the Sumerian-based SVM so that we can do our own manual inspection to verify that there is nothing clearly suspicious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumerian_texts_ids = list(sumerian_id_archive_map.keys())\n",
    "\n",
    "random_indices = random.sample(range(len(sumerian_texts_ids)), 100)\n",
    "random_ids = [sumerian_texts_ids[index] for index in range(len(sumerian_texts_ids)) if index in random_indices]\n",
    "\n",
    "random_sample_ids = []\n",
    "random_sample_texts = []\n",
    "random_sample_labels = []\n",
    "\n",
    "for text, t_list in sumerian_mapping.items():\n",
    "    for t in t_list:\n",
    "        if t.p_id in random_ids:\n",
    "            random_sample_ids.append(t.p_id)\n",
    "            random_sample_texts.append(text)\n",
    "            random_sample_labels.append(sumerian_id_archive_map[t.p_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results of random sample to CSV file\n",
    "\n",
    "sample = open('sumerian_random_sample.csv', 'w', encoding='utf-8')\n",
    "sample.write(\"PID,Text,Archive\\n\")\n",
    "for i in range(len(random_sample_ids)):\n",
    "# for i in range(1):\n",
    "    sample.write(random_sample_ids[i] + \",\" + random_sample_texts[i] + \",\" + random_sample_labels[i] + \"\\n\")\n",
    "sample.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now begin to deal with the issue of multiple transactions. Below, we will work on a way to identify which texts contain multiple transactions so that we can deal with them separately. \n",
    "\n",
    "We know that a multiple-transaction text dealing with animals will look something like this:\n",
    "\n",
    "1. number animal person-name [profession - optional]\n",
    "2. number animal person-name [profession - optional]\n",
    "\n",
    "\n",
    "The name and profession may appear on a separate line, in which case that line will not be numbered.\n",
    "\n",
    "An example of multiple-transaction animal texts is P142790."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#project: epsd2/u3adm/p009',\n",
       " '#atf: use unicode',\n",
       " '#atf: lang sux',\n",
       " '@tablet',\n",
       " '@obverse',\n",
       " '1. 1(diš) sila₄ a₂-[...]',\n",
       " '#lem: n; sila[lamb]; u',\n",
       " '2. 1(diš) sila₄ ur-nigar#[{gar}]',\n",
       " '#lem: n; sila[lamb]; PN',\n",
       " '3. 1(diš) sila₄ ur-x-[...]',\n",
       " '#lem: n; sila[lamb]; PN',\n",
       " '4. 4(diš) udu niga 1(diš) sila₄#',\n",
       " '#lem: n; udu[sheep]; niga[fattened]; n; sila[lamb]',\n",
       " '5. 3(diš) udu niga 1(diš) sila₄',\n",
       " '#lem: n; udu[sheep]; niga[fattened]; n; sila[lamb]',\n",
       " '6. ur-{d}lugal-banda₃{da}',\n",
       " '#lem: PN',\n",
       " '7. 1(diš) sila₄ lu₂-{d}asar-lu₂-hi',\n",
       " '#lem: n; sila[lamb]; PN',\n",
       " '@reverse',\n",
       " '1. 1(diš) maš₂ lu₂-dingir-ra',\n",
       " '#lem: n; maš[goat]; PN',\n",
       " '2. 1(diš) sila₄# bi₂-i₃-bi₂-a',\n",
       " '#lem: n; sila[lamb]; PN',\n",
       " '3. 1(diš) sila₄ gu₃-de₂-a',\n",
       " '#lem: n; sila[lamb]; PN',\n",
       " '4. 1(diš) sila₄ za-ni-a',\n",
       " '#lem: n; sila[lamb]; PN',\n",
       " '5. 2(diš) maš₂ ur-{d}en-lil₂-la₂',\n",
       " '#lem: n; maš[goat]; PN',\n",
       " '6. 1(diš) sila₄ ur-{d}nin-a-zu',\n",
       " '#lem: n; sila[lamb]; PN',\n",
       " '7. 4(diš) amar maš-da₃ šeš-kal-la',\n",
       " '#lem: n; amar[young]; mašda[gazelle]; PN',\n",
       " '8. u₄ 1(diš)-kam',\n",
       " '#lem: ud[sun]; n',\n",
       " '9. mu-kuₓ(DU)#',\n",
       " '#lem: mu.DU[delivery]',\n",
       " '10. ab-ba-sa₆-ga [...]',\n",
       " '#lem: PN; AN',\n",
       " '11. iti maš-da₃ [gu₇]',\n",
       " '#lem: itud[moon]; mašda[gazelle]; gu[eat]',\n",
       " '12. mu us₂-sa {d}[x]-{d}suen [...]',\n",
       " '#lem: mu[year]; us[follow]; PN; AN',\n",
       " '@left',\n",
       " '1. 2(u) 4(diš)',\n",
       " '#lem: n; n']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this to visually inspect different texts while working\n",
    "# This can help verify whether or not certain regex expressions work properly\n",
    "t = None\n",
    "for tr in all_transactions:\n",
    "    if tr.p_id == \"P142790\":\n",
    "        t = tr\n",
    "        break\n",
    "t.lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4. 4(diš) udu niga 1(diš) sila₄#\\n#lem: n; udu[sheep]; niga[fattened]; n; sila[lamb]\\n',\n",
       "  ''),\n",
       " ('5. 3(diš) udu niga 1(diš) sila₄\\n#lem: n; udu[sheep]; niga[fattened]; n; sila[lamb]\\n',\n",
       "  '')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First attempt - unfinished\n",
    "use = [line for line in t.lines if line[:5] == \"#lem:\" or line[0] not in \"#@\"]\n",
    "use_str = \"\\n\".join(use)\n",
    "re.findall(\"([1-9]\\. [1-9]+\\(.*\\) (\\n)?.*\\n#lem: n; .+\\[.+\\]\\n)\", use_str)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
