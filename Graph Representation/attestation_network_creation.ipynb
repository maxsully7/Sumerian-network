{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'sumerian_archive_map.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-377831011cc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# reading archive_map csv data into people_Drehem.csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0marchives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sumerian_archive_map.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#archives[archives.PID=='P'+'109555'].Archive.iloc[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpeople\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"people_Drehem.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'sumerian_archive_map.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# reading archive_map csv data into people_Drehem.csv\n",
    "\n",
    "archives = pd.read_csv(\"sumerian_archive_map.csv\")\n",
    "#archives[archives.PID=='P'+'109555'].Archive.iloc[0]\n",
    "people = pd.read_csv(\"people_Drehem.csv\")\n",
    "\n",
    "\n",
    "archives = pd.read_csv(\"archive_map.csv\")\n",
    "\n",
    "archives['PID'] = [int(archives['PID'][i][1:]) for i in np.arange(len(archives['PID']))]\n",
    "archives\n",
    "# archives[archives['PID'] == 104219]\n",
    "people = people.merge(archives, how='inner', left_on='p index', right_on='PID')\n",
    "people.to_csv(\"people_Archives_Drehem.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# container to store row data\n",
    "class Node:\n",
    "    def __init__(self, name, norm_name, role, profession, family, p_index, year, processed, archive):\n",
    "        self.name = name\n",
    "        self.norm_name = norm_name\n",
    "        self.role = role\n",
    "        self.profession = profession\n",
    "        self.family = family\n",
    "        self.p_index = p_index\n",
    "        self.year = year\n",
    "        self.processed = processed\n",
    "        self.archive = archive\n",
    "        self.id = None\n",
    "\n",
    "    def add_id(self, id):\n",
    "        self.id = id\n",
    "\n",
    "\n",
    "# traverses through people_Drehem.csv to create a node per row\n",
    "\n",
    "def create_nodes_list():\n",
    "    \n",
    "    with open('people_Archives_Drehem.csv', 'rt', encoding=\"utf-8\") as csvfile:\n",
    "\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        node_list = list()\n",
    "        for row in reader:\n",
    "            name = row['name']\n",
    "            norm_name = row['normalized name']\n",
    "            role = row['roles']\n",
    "            profession = row['profession']\n",
    "            family = row['family']\n",
    "            p_index = row['p index']\n",
    "            year = row['date name']\n",
    "            processed = row['processed date']\n",
    "            archive = row['Archive']\n",
    "            node = Node(name, norm_name, role, profession, family, p_index, year, processed, archive)\n",
    "            node_list.append(node)\n",
    "            \n",
    "    return node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_nodes_list(list_of_nodes):\n",
    "    with open('attestation_nodes.csv', 'w') as csvfile:\n",
    "        fieldnames = ['id', 'name','norm_name', 'role', 'profession', 'processed year', 'archive', 'family', 'p_index', 'date name']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        curr_id = 1\n",
    "\n",
    "        for node in list_of_nodes:\n",
    "            name = node.name\n",
    "            if type(node) is list:\n",
    "                node = node[0]\n",
    "\n",
    "            role = node.role\n",
    "            norm_name = node.norm_name\n",
    "            profession = node.profession\n",
    "            family = node.family\n",
    "            p_index = node.p_index\n",
    "            year = node.year\n",
    "            archive = node.archive\n",
    "            \n",
    "            # if there is no processed year data, sets Max and MinYear to be 0\n",
    "            # these nodes will be ignored in the final timeline visualization\n",
    "            processed = node.processed\n",
    "            if len(processed)>0:\n",
    "                maxYear = max(processed)\n",
    "                minYear = min(processed)\n",
    "            else:\n",
    "                maxYear = 0\n",
    "                minYear = 0\n",
    "            \n",
    "            maxDiff = 0\n",
    "            gStart = 0 # start year of largest gap between transactions for each node\n",
    "            gEnd = 0 # end year of largest gap between transactions for each node\n",
    "            pre_GapInstanceCount =  0\n",
    "            post_GapInstanceCount = 0\n",
    "            node.add_id(curr_id)\n",
    "            \n",
    "            #calculating proportions for Archives\n",
    "            prop_doa = 100*np.count_nonzero(np.array(archive)=='domesticated animal')/len(archive)\n",
    "            prop_dea = 100*np.count_nonzero(np.array(archive)=='dead animal')/len(archive)\n",
    "            prop_lo = 100*np.count_nonzero(np.array(archive)=='leather object')/len(archive)\n",
    "            prop_po = 100*np.count_nonzero(np.array(archive)=='precious object')/len(archive)\n",
    "            prop_q = 100*np.count_nonzero(np.array(archive)=='queen')/len(archive)\n",
    "            prop_s = 100*np.count_nonzero(np.array(archive)=='seal')/len(archive)\n",
    "            prop_u = 100*np.count_nonzero(np.array(archive)=='unknown')/len(archive)\n",
    "            prop_wa = 100*np.count_nonzero(np.array(archive)=='wild animal')/len(archive)\n",
    "            prop_w = 100*np.count_nonzero(np.array(archive)=='wool')/len(archive)\n",
    "\n",
    "            writer.writerow({\n",
    "                'id': curr_id,\n",
    "                'name': name,\n",
    "                'norm_name':norm_name,\n",
    "                'role': role,\n",
    "                'profession': profession,\n",
    "                'family': family,\n",
    "                'p_index': p_index,\n",
    "                'date name': year,\n",
    "                'archive':archive,\n",
    "                'processed year': processed,\n",
    "                })\n",
    "            curr_id += 1\n",
    "\n",
    "        csvfile.close()\n",
    "        \n",
    "        #roles = ['source', 'intermediary', 'recipient', 'source', 'representative']\n",
    "        #people_filt =  pd.read_csv('archive_proportions.csv')\n",
    "        #people_roles = people_filt['role']\n",
    "        #for r in roles:\n",
    "        #    people_filt[\"% \" + r] = people_roles.map(lambda a: a.count(r)/(a.count(\",\")+1))\n",
    "        #people_filt.to_csv('new_nodes.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_new_person_to_info(list_of_nodes):\n",
    "    p_indexes_to_people = defaultdict(list)\n",
    "    for node in list_of_nodes:\n",
    "        role = node.role\n",
    "        p_index = node.p_index\n",
    "        year = node.year\n",
    "        p_indexes_to_people[p_index].append(node)\n",
    "        \n",
    "    return p_indexes_to_people\n",
    "\n",
    "\n",
    "def create_edge_list(id_to_people):\n",
    "    with open('attestation_edges.csv', 'w') as csvfile:\n",
    "        fieldnames = ['id', 'source', 'target', 'p_index', 'year', 'type', 'source\\'s role', 'target\\'s role']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        curr_id = 1\n",
    "        for key, value in id_to_people.items():\n",
    "            if len(value) == 2:\n",
    "                list_of_roles = []\n",
    "                role_to_node = {}\n",
    "                node1 = None\n",
    "                node2 = None\n",
    "\n",
    "                for node in value:\n",
    "                    #print(type(node.role))\n",
    "                    list_of_roles.append(node.role)\n",
    "                    role_to_node[node.role] = node\n",
    "                    \n",
    "               # print(list_of_roles)    \n",
    "                if \"{'source'}\" in list_of_roles:\n",
    "                    role1 = \"{'source'}\"\n",
    "                    person1 = (role_to_node[role1]).name\n",
    "                    node1 = role_to_node[role1]\n",
    "                    id1 = (role_to_node[role1]).id\n",
    "\n",
    "                    list_of_roles.remove(\"{'source'}\")\n",
    "\n",
    "                    role2 = list_of_roles[0]\n",
    "                    person2 = (role_to_node[role2]).name\n",
    "                    node2 = role_to_node[role2]\n",
    "                    id2 = (role_to_node[role2]).id\n",
    "\n",
    "                    edge_type = \"Directed\"\n",
    "\n",
    "                # there are 456 transactions between other person -> \"recipient\"\n",
    "                # thus this creates 456 edges\n",
    "                # sometimes there are \"recipients\" -> \"recipients\" in a transactions, so the edge is chose arbitrarily\n",
    "                elif \"{'recipient'}\" in list_of_roles:\n",
    "                    role2 = \"{'recipient'}\"\n",
    "                    node2 = role_to_node[role2]\n",
    "                    person2 = node2.name\n",
    "                    id2 = node2.id\n",
    "\n",
    "                    list_of_roles.remove(\"{'recipient'}\")\n",
    "\n",
    "                    role1 = list_of_roles[0]\n",
    "                    node1 = role_to_node[role1]\n",
    "                    person1 = node1.name\n",
    "                    id1 = node1.id\n",
    "\n",
    "                    edge_type = \"Directed\"\n",
    "\n",
    "                # there are 280 transactions where there are neither a source nor a recipient\n",
    "                # thus this creates 280 edges\n",
    "                else:\n",
    "                    role1 = list_of_roles[0]\n",
    "                    person1 = (role_to_node[role1]).name\n",
    "                    node1 = (role_to_node[role1])\n",
    "                    id1 = node1.id\n",
    "\n",
    "                    role2 = list_of_roles[1]\n",
    "                    person2 = (role_to_node[role2]).name\n",
    "                    node2 = (role_to_node[role2])\n",
    "                    id2 = node2.id\n",
    "\n",
    "                    edge_type = \"Undirected\"\n",
    "\n",
    "                edge_id = curr_id\n",
    "                curr_id += 1\n",
    "                p_index = key\n",
    "                #index_of_year = node1.p_index.index(p_index)\n",
    "                #modified 10/5\n",
    "                year = node1.year\n",
    "                writer.writerow({\n",
    "                            'id': edge_id,\n",
    "                            'source': id1,\n",
    "                            'target': id2,\n",
    "                            'p_index': p_index,\n",
    "                            'year': year,\n",
    "                            'type': edge_type,\n",
    "                            'source\\'s role': role1,\n",
    "                            'target\\'s role': role2\n",
    "                            })\n",
    "                \n",
    "            elif len(value) == 3:\n",
    "                list_of_roles = []\n",
    "                role_to_node_list = defaultdict(list)\n",
    "                node1 = None\n",
    "                node2 = None\n",
    "                node3 = None\n",
    "\n",
    "                for node in value:\n",
    "                    list_of_roles.append(node.role)\n",
    "                    role_to_node_list[node.role].append(node)\n",
    "\n",
    "                if \"{'source'}\" in list_of_roles and \"{'recipient'}\" in list_of_roles:\n",
    "                    # source, recipient1, recipient2: source -> recipient1, source -> recipient2 (creates 2 edges)\n",
    "                    # there are 196 transactions of this structure\n",
    "                    # thus this creates 392 edges\n",
    "                    #print(\"I am a p-index with more than 3 nodes associated with me.\")\n",
    "                    if list_of_roles.count(\"{'recipient'}\") == 2:\n",
    "                        role1 = \"{'source'}\"\n",
    "                        #person1 = role_to_node_list[role1][0]\n",
    "                        node1 = role_to_node_list[role1][0]\n",
    "                        id1 = node1.id\n",
    "\n",
    "                        role2 = \"{'recipient'}\"\n",
    "                        #person2 = role_to_node_list[role2][0][0]\n",
    "                        node2 = role_to_node_list[role2][0]\n",
    "                        id2 = node2.id\n",
    "\n",
    "                        role3 = \"{'recipient'}\"\n",
    "                        #person3 = role_to_node_list[role2][1][0]\n",
    "                        node3 = role_to_node_list[role2][1]\n",
    "                        id3 = node3.id\n",
    "\n",
    "                        edge_type = \"Directed\"\n",
    "                        edge_id = curr_id\n",
    "                        curr_id += 1\n",
    "                        p_index = key\n",
    "                        #index_of_year = node1.p_index.index(p_index)\n",
    "                        year = node1.year\n",
    "\n",
    "                        writer.writerow({\n",
    "                            'id': edge_id,\n",
    "                            'source': id1,\n",
    "                            'target': id2,\n",
    "                            'p_index': p_index,\n",
    "                            'year': year,\n",
    "                            'type': edge_type,\n",
    "                            'source\\'s role': role1,\n",
    "                            'target\\'s role': role2\n",
    "                            })\n",
    "                        edge_id = curr_id\n",
    "                        curr_id += 1\n",
    "\n",
    "                        writer.writerow({\n",
    "                            'id': edge_id,\n",
    "                            'source': id1,\n",
    "                            'target': id2,\n",
    "                            'p_index': p_index,\n",
    "                            'year': year,\n",
    "                            'type': edge_type,\n",
    "                            'source\\'s role': role1,\n",
    "                            'target\\'s role': role2\n",
    "                            })\n",
    "                    else:\n",
    "                        role1 = \"{'source'}\"\n",
    "                        node1 = role_to_node_list[role1][0]\n",
    "                        id1 = node1.id\n",
    "\n",
    "                        list_of_roles.remove(\"{'source'}\")\n",
    "                        list_of_roles.remove(\"{'recipient'}\")\n",
    "\n",
    "                        role2 = list_of_roles[0]\n",
    "                        #person2 = role_to_node_list[role2][0][0]\n",
    "                        node2 = role_to_node_list[role2][0]\n",
    "                        id2 = node2.id\n",
    "\n",
    "                        role3 = \"{'recipient'}\"\n",
    "                        #person3 = role_to_node_list[role3][0][0]\n",
    "                        node3 = role_to_node_list[role3][0]\n",
    "                        id3 = node3.id\n",
    "\n",
    "                        edge_type = \"Directed\"\n",
    "                        edge_id = curr_id\n",
    "                        curr_id += 1\n",
    "                        p_index = key\n",
    "                        #index_of_year = node1.p_index.index(p_index)\n",
    "                        year = node1.year\n",
    "\n",
    "                        writer.writerow({\n",
    "                            'id': edge_id,\n",
    "                            'source': id1,\n",
    "                            'target': id2,\n",
    "                            'p_index': p_index,\n",
    "                            'year': year,\n",
    "                            'type': edge_type,\n",
    "                            'source\\'s role': role1,\n",
    "                            'target\\'s role': role2\n",
    "                            })\n",
    "                        edge_id = curr_id\n",
    "                        curr_id += 1\n",
    "                        writer.writerow({\n",
    "                            'id': edge_id,\n",
    "                            'source': id1,\n",
    "                            'target': id3,\n",
    "                            'p_index': p_index,\n",
    "                            'year': year,\n",
    "                            'type': edge_type,\n",
    "                            'source\\'s role': role1,\n",
    "                            'target\\'s role': role3\n",
    "                            })\n",
    "                        edge_id = curr_id\n",
    "                        curr_id += 1\n",
    "                        writer.writerow({\n",
    "                            'id': edge_id,\n",
    "                            'source': id2,\n",
    "                            'target': id3,\n",
    "                            'p_index': p_index,\n",
    "                            'year': year,\n",
    "                            'type': edge_type,\n",
    "                            'source\\'s role': role2,\n",
    "                            'target\\'s role': role3\n",
    "                            })\n",
    "            elif len(value) > 3:\n",
    "                list_of_roles = []\n",
    "                node_list = []\n",
    "                for node in value:\n",
    "                    list_of_roles.append(node.role)\n",
    "                    node_list.append(node)\n",
    "\n",
    "                role1 = list_of_roles[0]\n",
    "                node1 =  node_list[0]\n",
    "                id1 = node1.id\n",
    "\n",
    "                role2 = list_of_roles[1]\n",
    "                node2 = node_list[1]\n",
    "                id2 = node2.id\n",
    "\n",
    "                role3 = list_of_roles[2]\n",
    "                node3 =  node_list[2]\n",
    "                id3 = node3.id\n",
    "\n",
    "                edge_type = \"Undirected\"\n",
    "                edge_id = curr_id\n",
    "                curr_id += 1\n",
    "                p_index = key\n",
    "                index_of_year = node1.p_index\n",
    "                year = node1.year\n",
    "\n",
    "                writer.writerow({\n",
    "                    'id': edge_id,\n",
    "                    'source': id1,\n",
    "                    'target': id2,\n",
    "                    'p_index': p_index,\n",
    "                    'year': year,\n",
    "                    'type': edge_type,\n",
    "                    'source\\'s role': role1,\n",
    "                    'target\\'s role': role2\n",
    "                    })\n",
    "                edge_id = curr_id\n",
    "                curr_id += 1\n",
    "                writer.writerow({\n",
    "                    'id': edge_id,\n",
    "                    'source': id1,\n",
    "                    'target': id3,\n",
    "                    'p_index': p_index,\n",
    "                    'year': year,\n",
    "                    'type': edge_type,\n",
    "                    'source\\'s role': role1,\n",
    "                    'target\\'s role': role3\n",
    "                    })\n",
    "                edge_id = curr_id\n",
    "                curr_id += 1\n",
    "                writer.writerow({\n",
    "                    'id': edge_id,\n",
    "                    'source': id2,\n",
    "                    'target': id3,\n",
    "                    'p_index': p_index,\n",
    "                    'year': year,\n",
    "                    'type': edge_type,\n",
    "                    'source\\'s role': role2,\n",
    "                    'target\\'s role': role3\n",
    "                    })\n",
    "\n",
    "\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merges attestation_nodes with commodities information \n",
    "#code from Commodities and Cohorts + DBSCAN.ipynb\n",
    "def merge_with_commodities():\n",
    "    node_dt = pd.read_csv(\"attestation_nodes.csv\")\n",
    "    com1 = pd.read_csv(\"commodities1.csv\")\n",
    "    com2 = pd.read_csv(\"commodities2.csv\")\n",
    "    commodities = pd.concat([com1[['ip','commodities']], com2[['ip','commodities']]])\n",
    "    nodes_and_comm = pd.merge(node_dt, commodities, how = 'inner', left_on = 'p_index', right_on = 'ip').drop(['ip', 'id'], axis=1)\n",
    "    nodes_and_comm.to_csv(\"attestation_nodes.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    node_list = create_nodes_list()\n",
    "    create_new_nodes_list(node_list)\n",
    "    id_people = fill_new_person_to_info(node_list)\n",
    "    create_edge_list(id_people)\n",
    "    #merge with commodities tables\n",
    "    merge_with_commodities()\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
