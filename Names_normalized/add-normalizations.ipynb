{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from collections import Counter\n",
    "import operator\n",
    "import argparse\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions for running this as a regular .py from the command line (in case it will ever be necessary to combine scripts by using bash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFrom the command line:\\n\\n   python add-normalizations.py -i [input csvs] -j [ogsl json] -a [atf file] -l [output log]\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "From the command line:\n",
    "\n",
    "   python add-normalizations.py -i [input csvs] -j [ogsl json] -a [atf file] -l [output log]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define default files and their paths. This script requires atf-file with normalized names; json that contain OGSL sign list; input files i.e. the filtered .csv files. Log file is used for storing mismatching names for debugging.\n",
    "\n",
    "Also define sign and name dictionaries, define tags that are being fetched from the .csv. Generate also tags with [] in the beginning for later filtering purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default files and paths\n",
    "atf = '../Names_normalized/Ur 03 PD PNs.atf'\n",
    "json_ = '../ogsl/ogsl-sl.json' \n",
    "input_ = ['../filtered_%i_with_neighbors.csv' % i for i in range(1,11)] \n",
    "log = 'add-normalizations.log'\n",
    "\n",
    "# Sign and name normalization dicts\n",
    "SIGNS = {}\n",
    "NAMES = {}\n",
    "TAGS = ('PN', 'DN', 'RN', 'GN', 'SN', 'TN') #, 'TN', 'WN', 'RN', 'MN', 'LN', 'EN', 'ON', 'KN')\n",
    "BRTAGS = tuple(['[]%s' % t for t in TAGS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic IO-operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(filename):\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as data:\n",
    "        if filename.endswith('.json'):\n",
    "            return json.load(data)\n",
    "        else:\n",
    "            return data.read().splitlines()\n",
    "\n",
    "def writefile(filename, text):\n",
    "    with open(filename, 'w', encoding='utf-8') as data:\n",
    "        data.write(text)\n",
    "    print('wrote\\t%s' % filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define command line arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    ap = argparse.ArgumentParser(description='Test')\n",
    "    ap.add_argument('-j', '--json')\n",
    "    ap.add_argument('-i', '--input', nargs='*')\n",
    "    ap.add_argument('-a', '--atf')\n",
    "    ap.add_argument('-l', '--log')\n",
    "    return ap.parse_args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization functions.\n",
    "\n",
    "    normalize() removes determinatives and does basic transliteration conversion operations.\n",
    "    normalize_atf() changes pos-tags and signs that have different indices (e.g. ₓ -> ₁), fix tabs\n",
    "    remove_tags() removes all name pos-tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(string):\n",
    "    string = re.sub('\\{.+?\\}', '', string.lower())\n",
    "    string = string.replace('ṣ', 'sy')\n",
    "    SUB = str.maketrans(\"₀₁₂₃₄₅₆₇₈₉šŋ\", \"0123456789cg\")\n",
    "    return string.translate(SUB)\n",
    "\n",
    "def normalize_atf(string):\n",
    "    string = string.replace('[]SN', '[]GN')\n",
    "    string = string.replace('nigarX', 'nigar')\n",
    "    string = re.sub('\\t+', '\\t', string)\n",
    "    return string\n",
    "\n",
    "def remove_tags(string):\n",
    "    return re.sub('\\[\\][A-Z]N', '', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map transliterated words into abstract sign values, for example lugal-uru-da and lugal-iri-da will become LUGAL^IRI^DA as IRI has readings uru and iri.\n",
    "\n",
    "Sub-function hyphenate() normalizes all sign separators into dashes. This will help splitting transliterated words into individual signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_signs(xlit):\n",
    "    \"\"\" Map all sign values in string to name representation, i.e.\n",
    "    lugal-uru-da --> LUGAL^IRI^DA \"\"\"\n",
    "    def hyphenate(s):\n",
    "        return re.sub('[\\.:-]', '-', normalize(s))\n",
    "    \n",
    "    return '^'.join([SIGNS.get(sign, '$') for sign in hyphenate(xlit).split('-')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates frequencies for different mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_log(mismatches):\n",
    "    return ['%i\\t%s' % (v, k) for k, v in sorted(Counter(mismatches).items(),\n",
    "                                                 key=operator.itemgetter(1))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATF parser. Parsing includes following steps:\n",
    "\n",
    "    1. define typical sumerian morphemes and their surface representations\n",
    "    2. iterate ATF line by line and pass it throught normalize_atf()\n",
    "    3. fetch transliteration and normalization\n",
    "    4. use BRTAGS for matching only those names that contain wanted pos-tags\n",
    "    5. remove tags from normalizations\n",
    "    6. use map_signs() function to convert transliteration into abstract sign representation.\n",
    "    7. generate three copies of each transliterated name:\n",
    "        A) a copy with typical morphemes removed (lugal-dúr-ta --> lugal-dúr); morphemic endings are only removed\n",
    "        if the the surface representations of morphemes are not found from the normalized name: e.g. PN-ke4 has its\n",
    "        /ak+e/ = ke4 removed only if the normalization does not end in /ke/. This should ensure that this process\n",
    "        does not produce ambiguity.\n",
    "        B) a copy with typical morphemes added (lugal-dúr --> lugal-dúr-ta, lugal-dúr-ra etc.); this tries to ensure\n",
    "        that the name dictionary will have different morphological forms for names and we do not have to do \n",
    "        morphological parsing in the CSV file where surface forms are not present (and we cannot know if e.g. -ta\n",
    "        is a part of the name or not). This produces impossible morpheme combinations for names that already have\n",
    "        case endings, but it should not matter as they will not match with anything; e.g. lugal-dúr-ta-ta.\n",
    "        C) original transliteration (lugal-dúr)\n",
    "        \n",
    "        save these representations in NAMES dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_atf(filename):\n",
    "\n",
    "    morphs = {'-ta': 'ta',\n",
    "              '-ce3': 'še',\n",
    "              '-ra': 'ra',\n",
    "              '-ke4': 'ke',\n",
    "              '-ar': 'ar'}\n",
    "\n",
    "    for line in readfile(filename):\n",
    "        line = normalize_atf(line)\n",
    "        elems = line.split('\\t')\n",
    "        if len(elems) > 2:\n",
    "            _, xlit, norm = elems[0:3]\n",
    "            if norm.endswith(BRTAGS):\n",
    "                \"\"\" Remove or add morphology \"\"\"\n",
    "                #norm = remove_tags(norm)\n",
    "                for morph in morphs:\n",
    "                    if xlit.endswith(morph) and not norm.endswith(morphs[morph]):\n",
    "                        key = map_signs(xlit.rstrip(morph))\n",
    "                        NAMES[key] = norm\n",
    "                \n",
    "                    key = map_signs(xlit + morph)\n",
    "                    NAMES[key] = norm\n",
    "\n",
    "                \"\"\" Add name as it is \"\"\"\n",
    "                key = map_signs(xlit)\n",
    "                NAMES[key] = norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON parser. Read OGSL sign list in JSON format to Python dict and normalize them to match the ATF file by using normalize() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(filename):\n",
    "    \"\"\" Build a sign dictionary based on JSON file by mapping\n",
    "    normalized sign values to value(SIGN) notation \"\"\"\n",
    "    ogsl = readfile(filename)\n",
    "    for sign in ogsl['signs'].keys():\n",
    "        vals = ogsl['signs'][sign].get('values', None)\n",
    "        if vals is not None:\n",
    "            for v in vals:\n",
    "                SIGNS[normalize(v)] = sign\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV parser. CSV parsing includes following steps\n",
    "\n",
    "    1. check if there are multiple input files or just one. If one, make sure it will not be a str().\n",
    "    2. iterate all lines in every file\n",
    "    3. define header for new CSV files\n",
    "    4. split CSV by commas\n",
    "    5. fetch all names that have predefined TAGS\n",
    "    6. normalize transliteration by using normalize() and remove_tags()\n",
    "    7. use map_signs() to convert transliteration into abstract sign value representation\n",
    "    8. try to find corresponding abstract representation of the name from NAMES dictionary; if it cannot be found,\n",
    "       return empty string\n",
    "    9. append found normalizations into CSV between lemma and id_text\n",
    "    10. write new CSV file (same path as the input, but mark them with _normalized suffix)\n",
    "\n",
    "In the end, write a log file that contains all mismatches and their frequencies. Print a short summary of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv(filename):\n",
    "    \"\"\" Read all input CSV-files. Normalize all transliterated names \"\"\"\n",
    "    no = 0\n",
    "    mismatches = []\n",
    "    yes = 0\n",
    "    \n",
    "    if isinstance(filename, str):\n",
    "        filename = [filename]\n",
    "\n",
    "    for f in filename:\n",
    "        #,Unnamed: 0,lemma,id_text,id_line,id_word,label,prof?,role?,family?,number?,commodity?,P_Number,neighbors\n",
    "        csv_output = ['id,unnamed,lemma,normalizaton,id_text,id_line,id_word,label,prof?,role?,family?,number?,commodity?,P_Number,neighbors']\n",
    "        for line in readfile(f)[1:]:\n",
    "            norm = ''\n",
    "            fields = line.split(',')\n",
    "            id_ = fields[0]\n",
    "            unnamed = fields[1]\n",
    "            lemma = fields[2]\n",
    "            rest = fields[3:]\n",
    "            if lemma.endswith(TAGS):\n",
    "                key = map_signs(normalize(remove_tags(lemma)))\n",
    "                norm = NAMES.get(key, '')\n",
    "                if not norm:\n",
    "                    no += 1\n",
    "                    mismatches.append(lemma)\n",
    "                else:\n",
    "                    yes += 1\n",
    "            csv_output.append(','.join([id_, unnamed, lemma, norm] + rest))\n",
    "        writefile(f.replace('with_neighbors', 'with_neighbors_normalized'), '\\n'.join(csv_output))\n",
    "\n",
    "    \"\"\" Print mismatches for debugging purposes \"\"\"\n",
    "    writefile(log, '\\n'.join(make_log(mismatches)))\n",
    "\n",
    "    print('mismatches %i | unique %i | matches %i | see %s for info' %\\\n",
    "              (no, len(set(mismatches)), yes, log))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get command line arguments if given, else use predefined files and paths (apparently Jupyter does not like argparse, thus set commandline = True if there is need to use it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote\t../filtered_1_with_neighbors_normalized.csv\n",
      "wrote\t../filtered_2_with_neighbors_normalized.csv\n",
      "wrote\t../filtered_3_with_neighbors_normalized.csv\n",
      "wrote\t../filtered_4_with_neighbors_normalized.csv\n",
      "wrote\t../filtered_5_with_neighbors_normalized.csv\n",
      "wrote\t../filtered_6_with_neighbors_normalized.csv\n",
      "wrote\t../filtered_7_with_neighbors_normalized.csv\n",
      "wrote\t../filtered_8_with_neighbors_normalized.csv\n",
      "wrote\t../filtered_9_with_neighbors_normalized.csv\n",
      "wrote\t../filtered_10_with_neighbors_normalized.csv\n",
      "wrote\tadd-normalizations.log\n",
      "mismatches 1595 | unique 1067 | matches 69756 | see add-normalizations.log for info\n"
     ]
    }
   ],
   "source": [
    "commandline = False\n",
    "\n",
    "if commandline:\n",
    "    args = get_args()\n",
    "\n",
    "    if args.log is not None:\n",
    "        log = args.log\n",
    "\n",
    "    if args.json is None:\n",
    "        parse_json(json_)\n",
    "    else:\n",
    "        parse_json(args.json)\n",
    "\n",
    "    if args.atf is None:\n",
    "        parse_atf(atf)\n",
    "    else:\n",
    "        parse_atf(args.atf)\n",
    "\n",
    "    if args.input is None:\n",
    "        parse_csv(input_)\n",
    "    else:\n",
    "        parse_csv(args.input)\n",
    "else:\n",
    "    parse_json(json_)\n",
    "    parse_atf(atf)\n",
    "    parse_csv(input_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
